{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: torch in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (0.17.2+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\test\\anaconda3\\envs\\socialsignal\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.9/39.5 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.5/39.5 MB 31.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 19.7/39.5 MB 34.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 26.5/39.5 MB 34.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 33.6/39.5 MB 33.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.1/39.5 MB 32.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.5/39.5 MB 30.6 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# Install dependencies\n",
    "!pip install numpy pandas matplotlib torch torchvision scikit-learn\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from image_dataloader import SocialSignalDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 3, 112, 112])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing dataset\n",
    "# Transform images\n",
    "\n",
    "training_path = 'data/train'\n",
    "testing_path = 'data/testing'\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Grayscale(3), # RGB\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = SocialSignalDataset(root_dir=training_path, transform=img_transform)\n",
    "# change batch size accordingly (my pc can only run up to 16)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) \n",
    "\n",
    "# Check input size for forward feed network\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break  # We only want to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialSignalModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SocialSignalModel, self).__init__()\n",
    "        \"\"\"\"\n",
    "        input_channel: 3 or 4 // test which one gets better inputs\n",
    "            - 3: for RGB\n",
    "            - 4: +1 for Depth from camera\n",
    "        num_classes: 2 \n",
    "            0 for surprise, 1 for fear\n",
    "        \"\"\"\n",
    "        # TODO: IMPLEMENT\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        ) # Input size 128 * 14 * 14\n",
    "        self.in_features = 128 * 14 * 14 #Flattened size after 3 Conv2d Layers and Pooling\n",
    "        self.fc = nn.Linear(self.in_features, num_classes) # in_features = batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: IMPLEMENT\n",
    "        # CNN --> Conv2d(N,Cin,H,W) --> (N, C_out, H_out,W_out)\n",
    "        x = self.cnn(x)\n",
    "\n",
    "        # Flatten \n",
    "        x = x.view(-1,self.in_features)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "model = SocialSignalModel().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.6339, Accuracy: 65.71%\n",
      "Epoch [2/30], Loss: 0.5305, Accuracy: 72.86%\n",
      "Epoch [3/30], Loss: 0.5036, Accuracy: 67.14%\n",
      "Epoch [4/30], Loss: 0.5028, Accuracy: 75.71%\n",
      "Epoch [5/30], Loss: 0.4288, Accuracy: 81.43%\n",
      "Epoch [6/30], Loss: 0.4205, Accuracy: 84.29%\n",
      "Epoch [7/30], Loss: 0.3022, Accuracy: 87.14%\n",
      "Epoch [8/30], Loss: 0.3248, Accuracy: 85.71%\n",
      "Epoch [9/30], Loss: 0.1837, Accuracy: 94.29%\n",
      "Epoch [10/30], Loss: 0.1481, Accuracy: 94.29%\n",
      "Epoch [11/30], Loss: 0.0988, Accuracy: 97.14%\n",
      "Epoch [12/30], Loss: 0.0677, Accuracy: 98.57%\n",
      "Epoch [13/30], Loss: 0.0667, Accuracy: 98.57%\n",
      "Epoch [14/30], Loss: 0.1216, Accuracy: 94.29%\n",
      "Epoch [15/30], Loss: 0.0769, Accuracy: 97.14%\n",
      "Epoch [16/30], Loss: 0.0266, Accuracy: 98.57%\n",
      "Epoch [17/30], Loss: 0.0321, Accuracy: 98.57%\n",
      "Epoch [18/30], Loss: 0.0214, Accuracy: 100.00%\n",
      "Epoch [19/30], Loss: 0.0116, Accuracy: 100.00%\n",
      "Epoch [20/30], Loss: 0.0066, Accuracy: 100.00%\n",
      "Epoch [21/30], Loss: 0.0065, Accuracy: 100.00%\n",
      "Epoch [22/30], Loss: 0.0020, Accuracy: 100.00%\n",
      "Epoch [23/30], Loss: 0.0041, Accuracy: 100.00%\n",
      "Epoch [24/30], Loss: 0.0017, Accuracy: 100.00%\n",
      "Epoch [25/30], Loss: 0.0024, Accuracy: 100.00%\n",
      "Epoch [26/30], Loss: 0.0009, Accuracy: 100.00%\n",
      "Epoch [27/30], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [28/30], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [29/30], Loss: 0.0009, Accuracy: 100.00%\n",
      "Epoch [30/30], Loss: 0.0006, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# From Assignment 3 Training\n",
    "num_epochs =  30# set the number of epochs for training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Loop over each batch\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # if epoch%5 == 0: # change the saving frequency as you want\n",
    "    #     model_path = os.path.join('./model_configs', f'image_lstm_model_{epoch}.pth')\n",
    "    #     torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
